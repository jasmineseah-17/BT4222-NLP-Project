{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import bs4 as bs\n",
    "\n",
    "from lxml import html\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get CIK and DJIA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo_url = \"https://finance.yahoo.com/quote/%5EDJI/components/\"\n",
    "# djia_table = pd.read_html(yahoo_url, header=0, index_col=0)[0]\n",
    "# djia_table = djia_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# djia_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>SEC filings</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date first added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>reports</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>St. Paul, Minnesota</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AXP</td>\n",
       "      <td>American Express Co</td>\n",
       "      <td>reports</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Financials</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>30/6/1976</td>\n",
       "      <td>4962</td>\n",
       "      <td>1850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>reports</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Cupertino, California</td>\n",
       "      <td>30/11/1982</td>\n",
       "      <td>320193</td>\n",
       "      <td>1977.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA</td>\n",
       "      <td>Boeing Company</td>\n",
       "      <td>reports</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12927</td>\n",
       "      <td>1916.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAT</td>\n",
       "      <td>Caterpillar Inc.</td>\n",
       "      <td>reports</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Deerfield, Illinois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18230</td>\n",
       "      <td>1925.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol             Security SEC filings             GICS Sector  \\\n",
       "0    MMM           3M Company     reports             Industrials   \n",
       "1    AXP  American Express Co     reports              Financials   \n",
       "2   AAPL           Apple Inc.     reports  Information Technology   \n",
       "3     BA       Boeing Company     reports             Industrials   \n",
       "4    CAT     Caterpillar Inc.     reports             Industrials   \n",
       "\n",
       "        GICS Sub Industry  Headquarters Location Date first added     CIK  \\\n",
       "0             Industrials    St. Paul, Minnesota              NaN   66740   \n",
       "1              Financials     New York, New York        30/6/1976    4962   \n",
       "2  Information Technology  Cupertino, California       30/11/1982  320193   \n",
       "3             Industrials      Chicago, Illinois              NaN   12927   \n",
       "4             Industrials    Deerfield, Illinois              NaN   18230   \n",
       "\n",
       "   Founded  \n",
       "0   1902.0  \n",
       "1   1850.0  \n",
       "2   1977.0  \n",
       "3   1916.0  \n",
       "4   1925.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wiki_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "# cik_df = pd.read_html(wiki_url,header=0,index_col=0)[0]\n",
    "# cik_df['GICS Sector'] = cik_df['GICS Sector'].astype(\"category\")\n",
    "# cik_df['GICS Sub Industry'] = cik_df['GICS Sector'].astype(\"category\")\n",
    "# cik_df = cik_df.reset_index()\n",
    "# cik_df.head()\n",
    "cik_df = pd.read_csv(\"cik_mapper.csv\")\n",
    "cik_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SEC limits users to 10 requests per second, so we need to make sure we are not making requests too quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteLogFile(log_file_name, text):\n",
    "    \n",
    "    '''\n",
    "    Helper function.\n",
    "    Writes a log file with all notes and\n",
    "    error messages from a scraping \"session\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "    text : str\n",
    "        Text to write to the log file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    with open(log_file_name, \"a\") as log_file:\n",
    "        log_file.write(text)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname_10k = '../../Raw Data/Data_10K'\n",
    "pathname_10q = '../../Raw Data/Data_10Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape10K(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name, start_year=2010):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Ks for a particular CIK from EDGAR.\n",
    "    \n",
    "    Args\n",
    "        browse_url_base : str\n",
    "            Base URL for browsing EDGAR.\n",
    "        filing_url_base : str\n",
    "            Base URL for filings listings on EDGAR.\n",
    "        doc_url_base : str\n",
    "            Base URL for one filing's document tables\n",
    "            page on EDGAR.\n",
    "        cik : str\n",
    "            Central Index Key.\n",
    "        log_file_name : str\n",
    "            Name of the log file (should be a .txt file).\n",
    "        start_year : int\n",
    "            The beginning year in which scraping will start from\n",
    "        \n",
    "    Returns\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Check if we've already scraped this CIK\n",
    "    try:\n",
    "        os.mkdir(cik)\n",
    "    except OSError:\n",
    "        print(\"Already scraped CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    # If we haven't, go into the directory for that CIK\n",
    "    os.chdir(cik)\n",
    "    \n",
    "    print('Scraping CIK', cik)\n",
    "    \n",
    "    page = 0\n",
    "    final_df = pd.DataFrame()\n",
    "    url = browse_url_base % cik\n",
    "    while True:\n",
    "        print('---', page, '---')\n",
    "        \n",
    "        # Request list of 10-K filings\n",
    "        res = requests.get(url)\n",
    "        \n",
    "        # If the request failed, log the failure and exit\n",
    "        if res.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            os.rmdir(cik) # remove empty dir\n",
    "            text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            return\n",
    "        # If the request doesn't fail, continue...\n",
    "        \n",
    "        # Parse the response HTML using BeautifulSoup\n",
    "        soup = bs.BeautifulSoup(res.text, 'lxml')\n",
    "        \n",
    "        # Extract all tables from the response\n",
    "        html_tables = soup.find_all('table')\n",
    "        \n",
    "        # Check that the table we're looking for exists\n",
    "        # If it doesn't, exit\n",
    "        if len(html_tables)<3:\n",
    "            print(\"table too short\")\n",
    "            os.chdir('..')\n",
    "            return\n",
    "\n",
    "        # Parse the Filings table\n",
    "        filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "        filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "        final_df = final_df.append(filings_table)\n",
    "        final_df = final_df.reset_index().drop([\"index\"], axis=1)\n",
    "        final_df[\"Filing Date\"] = pd.to_datetime(final_df[\"Filing Date\"])\n",
    "        final_df[\"year\"] = final_df[\"Filing Date\"].dt.year\n",
    "        \n",
    "        if final_df.iloc[-1][\"year\"] < start_year:\n",
    "            final_df = final_df[final_df[\"year\"] >= start_year]\n",
    "            final_df = final_df.drop([\"year\"], axis=1)\n",
    "            break\n",
    "\n",
    "        if len(soup.find_all(\"td\")[-1].find_all(\"input\")) == 0:\n",
    "            break\n",
    "        \n",
    "        next_page = soup.find_all(\"td\")[-1].find_all(\"input\")[-1].get(\"onclick\")\n",
    "        flag = soup.find_all(\"td\")[-1].find_all(\"input\")[-1].get(\"value\").split()[0]\n",
    "        \n",
    "        if next_page:\n",
    "            if flag == 'Next':\n",
    "                next_page = next_page.split(\"parent.location=\")[1].replace('\"','').replace(\"'\", '')\n",
    "                url = \"https://www.sec.gov\" + next_page\n",
    "                page += 1\n",
    "                print(url)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    filings_table = final_df\n",
    "    filings_table[\"Filing Date\"] = filings_table[\"Filing Date\"].dt.date.map(lambda x:str(x))        \n",
    "\n",
    "    # Get only 10-K and 10-K405 document filings\n",
    "    filings_table = filings_table[(filings_table['Filings'] == '10-K') | (filings_table['Filings'] == '10-K405')]\n",
    "\n",
    "    # If filings table doesn't have any\n",
    "    # 10-Ks or 10-K405s, exit\n",
    "    if len(filings_table)==0:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no))\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing\n",
    "        if docs_page.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "\n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-K and 10-K405 entries for the filing\n",
    "        docs_table = docs_table[(docs_table['Type'] == '10-K') | (docs_table['Type'] == '10-K405')]\n",
    "        \n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        docname = docname.split()[0]\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            os.chdir('..')\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        \n",
    "        # Request the file\n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname))\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "        \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        \n",
    "    # Move back to the main 10-K directory\n",
    "    os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape10Q(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name, start_year=2010):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Qs for a particular CIK from EDGAR.\n",
    "    \n",
    "    Args\n",
    "        browse_url_base : str\n",
    "            Base URL for browsing EDGAR.\n",
    "        filing_url_base : str\n",
    "            Base URL for filings listings on EDGAR.\n",
    "        doc_url_base : str\n",
    "            Base URL for one filing's document tables\n",
    "            page on EDGAR.\n",
    "        cik : str\n",
    "            Central Index Key.\n",
    "        log_file_name : str\n",
    "            Name of the log file (should be a .txt file).\n",
    "        start_year : int\n",
    "            The beginning year in which scraping will start from\n",
    "        \n",
    "    Returns\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Check if we've already scraped this CIK\n",
    "    try:\n",
    "        os.mkdir(cik)\n",
    "    except OSError:\n",
    "        print(\"Already scraped CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    # If we haven't, go into the directory for that CIK\n",
    "    os.chdir(cik)\n",
    "    \n",
    "    print('Scraping CIK', cik)\n",
    "    \n",
    "    page = 0\n",
    "    final_df = pd.DataFrame()\n",
    "    url = browse_url_base % cik\n",
    "    while True:\n",
    "        print('---', page, '---')\n",
    "        \n",
    "        # Request list of 10-Q filings\n",
    "        res = requests.get(url)\n",
    "        \n",
    "        # If the request failed, log the failure and exit\n",
    "        if res.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            os.rmdir(cik) # remove empty dir\n",
    "            text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            return\n",
    "        # If the request doesn't fail, continue...\n",
    "        \n",
    "        # Parse the response HTML using BeautifulSoup\n",
    "        soup = bs.BeautifulSoup(res.text, 'lxml')\n",
    "        \n",
    "        # Extract all tables from the response\n",
    "        html_tables = soup.find_all('table')\n",
    "        \n",
    "        # Check that the table we're looking for exists\n",
    "        # If it doesn't, exit\n",
    "        if len(html_tables)<3:\n",
    "            print(\"table too short\")\n",
    "            os.chdir('..')\n",
    "            return\n",
    "\n",
    "        # Parse the Filings table\n",
    "        filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "        filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "        final_df = final_df.append(filings_table)\n",
    "        final_df = final_df.reset_index().drop([\"index\"], axis=1)\n",
    "        final_df[\"Filing Date\"] = pd.to_datetime(final_df[\"Filing Date\"])\n",
    "        final_df[\"year\"] = final_df[\"Filing Date\"].dt.year\n",
    "        \n",
    "        if final_df.iloc[-1][\"year\"] < start_year:\n",
    "            final_df = final_df[final_df[\"year\"] >= start_year]\n",
    "            final_df = final_df.drop([\"year\"], axis=1)\n",
    "            break\n",
    "            \n",
    "        if len(soup.find_all(\"td\")[-1].find_all(\"input\")) == 0:\n",
    "            break\n",
    "        \n",
    "        next_page = soup.find_all(\"td\")[-1].find_all(\"input\")[-1].get(\"onclick\")\n",
    "        flag = soup.find_all(\"td\")[-1].find_all(\"input\")[-1].get(\"value\").split()[0]\n",
    "\n",
    "        if next_page:\n",
    "            if flag == 'Next':\n",
    "                next_page = next_page.split(\"parent.location=\")[1].replace('\"','').replace(\"'\", '')\n",
    "                url = \"https://www.sec.gov\" + next_page\n",
    "                page += 1\n",
    "                print(url)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    filings_table = final_df\n",
    "    filings_table[\"Filing Date\"] = filings_table[\"Filing Date\"].dt.date.map(lambda x:str(x))        \n",
    "\n",
    "    # Get only 10-Q document filings\n",
    "    filings_table = filings_table[filings_table['Filings'] == '10-Q']\n",
    "\n",
    "    # If filings table doesn't have any\n",
    "    # 10-Qs, exit\n",
    "    if len(filings_table)==0:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no))\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing    \n",
    "        if docs_page.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "            \n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-Q entries for the filing\n",
    "        docs_table = docs_table[docs_table['Type'] == '10-Q']\n",
    "        \n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        docname = docname.split()[0]\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            os.chdir('..')\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        \n",
    "        # Request the file\n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname))\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "            \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        \n",
    "    # Move back to the main 10-Q directory\n",
    "    os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_df.CIK = cik_df.CIK.map(lambda x:str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the function to scrape 10-Ks\n",
    "\n",
    "# Define parameters\n",
    "browse_url_base_10k = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-K'\n",
    "filing_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Set correct directory\n",
    "os.chdir(pathname_10k)\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "time = strftime(\"%Y-%m-%d %Hh%Mm%Ss\", gmtime())\n",
    "log_file_name = 'log '+time+'.txt'\n",
    "with open(log_file_name, 'a') as log_file:\n",
    "    log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Ks\n",
    "for cik in tqdm(cik_df.CIK):\n",
    "    Scrape10K(browse_url_base=browse_url_base_10k, \n",
    "          filing_url_base=filing_url_base_10k, \n",
    "          doc_url_base=doc_url_base_10k, \n",
    "          cik=cik,\n",
    "          log_file_name=log_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../../Analytics/10K10Q/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the function to scrape 10-Qs\n",
    "\n",
    "# Define parameters\n",
    "browse_url_base_10q = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-Q'\n",
    "filing_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Set correct directory\n",
    "os.chdir(pathname_10q)\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "time = strftime(\"%Y-%m-%d %Hh%Mm%Ss\", gmtime())\n",
    "log_file_name = 'log '+time+'.txt'\n",
    "with open(log_file_name, 'a') as log_file:\n",
    "    log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Ks\n",
    "for cik in tqdm(cik_df.CIK):\n",
    "    Scrape10Q(browse_url_base=browse_url_base_10q, \n",
    "          filing_url_base=filing_url_base_10q, \n",
    "          doc_url_base=doc_url_base_10q, \n",
    "          cik=cik,\n",
    "          log_file_name=log_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
