{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import *\n",
    "from pandas_datareader.data import DataReader\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "nlp = spacy.load(\"C:/Users/ksjag/Anaconda3/Lib/site-packages/en_core_web_sm/en_core_web_sm-2.2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_url = \"https://finance.yahoo.com/quote/%5EDJI/components/\"\n",
    "djia_table = pd.read_html(yahoo_url, header=0, index_col=0)[0]\n",
    "djia_table = djia_table.reset_index()\n",
    "\n",
    "tickers = djia_table.Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2010-01-01\"\n",
    "end_date = \"2019-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDate(x):\n",
    "    return datetime.strptime(x[0:10], \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def get_data_for_multiple_stocks(tickers):\n",
    "    '''\n",
    "    Obtain stocks information (Date, OHLC, Volume and Adjusted Close). \n",
    "    Uses Pandas DataReader to make an API Call to Yahoo Finance and download the data directly.\n",
    "    Computes other values - Log Return and Arithmetic Return.\n",
    "    \n",
    "    Input: List of Stock Tickers\n",
    "    Output: A dictionary of dataframes for each stock\n",
    "    '''\n",
    "    stocks = dict()\n",
    "    for ticker in tickers:\n",
    "        s = DataReader(ticker, 'yahoo', start_date, end_date)\n",
    "        s.insert(0, \"Ticker\", ticker)  #insert ticker column so you can reference better later\n",
    "        s['Date'] = pd.to_datetime(s.index) #useful for transformation later\n",
    "        s['Adj Prev Close'] = s['Adj Close'].shift(1)\n",
    "        s['Log Return'] = np.log(s['Adj Close']/s['Adj Prev Close'])\n",
    "        s['Return'] = (s['Adj Close']/s['Adj Prev Close']-1)\n",
    "        s = s.reset_index(drop=True)\n",
    "        \n",
    "        cols = list(s.columns.values) # re-arrange columns\n",
    "        cols.remove(\"Date\")\n",
    "        s = s[[\"Date\"] + cols]\n",
    "        \n",
    "        stocks[ticker] = s\n",
    "        \n",
    "    return stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, ticker):\n",
    "\n",
    "    ### Make into proper time series like dataframe\n",
    "    df = this_df = pd.read_csv(\"../../Raw Data/Financial News/\" + ticker + \".csv\")\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df[\"Date\"] = df[\"Date\"].apply(getDate)\n",
    "    df.sort_values(by=\"Date\", inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.drop(columns=[\"num_hits\"], inplace=True)\n",
    "\n",
    "    # ## Named Entity Recognition to filter out non-company related stuff\n",
    "    # noun_or_not = []   ## store the pos_\n",
    "    # for row in range(len(df)):\n",
    "    #     this_headline = df.loc[row,\"main_headline\"]\n",
    "    #     this_doc  = nlp(this_headline)\n",
    "\n",
    "    #     done = False\n",
    "    #     for token in this_doc:\n",
    "    #         if str(token)[0:len(company)].lower() == company.lower():\n",
    "    #             noun_or_not.append(token.pos_)\n",
    "    #             done = True\n",
    "    #             break\n",
    "    #     if done == False:\n",
    "    #         noun_or_not.append(\"remove\")\n",
    "    # df = pd.concat([df.reset_index(drop=True), pd.DataFrame(noun_or_not, columns=[\"noun_or_not\"])], axis=1)\n",
    "    # df = df[df.noun_or_not == \"PROPN\"]\n",
    "    # df.drop([\"noun_or_not\"], axis=1, inplace=True)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ##### JOIN WITH PRICE HISTORY ######\n",
    "    start_date = \"2010-01-01\"\n",
    "    end_date = \"2019-12-31\"\n",
    "    stock_prices = get_data_for_multiple_stocks([ticker])[ticker]\n",
    "\n",
    "    stock_prices = stock_prices[[\"Date\", \"Adj Close\", \"Adj Prev Close\", \"Return\"]]\n",
    "    df = pd.merge(df, stock_prices, how='inner', on='Date')\n",
    "\n",
    "    df[\"text_label\"] = df[\"main_headline\"] + \". \" + df[\"absract\"]\n",
    "    df[\"Label\"] = 1\n",
    "    df.loc[df[\"Return\"] < 0, \"Label\"] = -1\n",
    "\n",
    "\n",
    "    ## LEMMATIZE ###############\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        return [''.join(lemmatizer.lemmatize(w, 'v')) for w in w_tokenizer.tokenize(text)]\n",
    "    def lemmatize_text_str(text):\n",
    "        string = ''\n",
    "        for w in w_tokenizer.tokenize(text):\n",
    "            string = string + ' ' + lemmatizer.lemmatize(w, 'v')\n",
    "        return string\n",
    "\n",
    "\n",
    "    df_filtered = df[[\"Date\", \"word_count\", \"text_label\", \"Label\", \"Return\"]]\n",
    "    df_filtered['text_lem_lst'] = df_filtered['text_label'].apply(lemmatize_text)\n",
    "    df_filtered['text_lem_str'] = df_filtered['text_label'].apply(lemmatize_text_str)\n",
    "\n",
    "\n",
    "    ### SENTIMENT SCORE ############\n",
    "    def detect_sentiment(text):    \n",
    "        # use this line instead for Python 3\n",
    "        blob = TextBlob(text)\n",
    "        return blob.sentiment.polarity\n",
    "\n",
    "    df_filtered[\"sentiment_txtblob\"] = df_filtered.text_lem_str.apply(detect_sentiment)\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df_filtered[\"sentiment_nltk\"] = df_filtered.text_lem_str.apply(lambda x: sid.polarity_scores(x))\n",
    "    df_filtered[\"positivity_sentiment_nltk\"] = df_filtered.sentiment_nltk.apply(lambda x: x[\"pos\"])\n",
    "    df_filtered[\"compound_sentiment_nltk\"] = df_filtered.sentiment_nltk.apply(lambda x: x[\"compound\"])\n",
    "    df_filtered[\"negativity_sentiment_nltk\"] = df_filtered.sentiment_nltk.apply(lambda x: x[\"neg\"])\n",
    "    df_filtered[\"neutral_sentiment_nltk\"] = df_filtered.sentiment_nltk.apply(lambda x: x[\"neu\"])\n",
    "    df_filtered.drop(columns=[\"sentiment_nltk\"], inplace=True)\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    continue  ## take this out to actually run\n",
    "    print(ticker)\n",
    "    \n",
    "    this_df = pd.read_csv(\"../../Raw Data/Financial News/\" + ticker + \".csv\")\n",
    "    company = djia_table[djia_table[\"Symbol\"] == ticker][\"Company Name\"]\n",
    "    \n",
    "    this_features = generate_features(this_df, ticker)\n",
    "    \n",
    "    this_features.to_csv(\"../../Processed Data/Financial News/\" + ticker + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each company, train a model from 2010 - 2018, and generate predictions for 2019, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_csv(ticker):\n",
    "    this_df = pd.read_csv(\"../../Processed Data/Financial News/\" + ticker + \".csv\")\n",
    "    this_df.drop_duplicates(subset=\"Date\", inplace=True, keep=\"first\")\n",
    "    this_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df_train = this_df[this_df[\"Date\"] < \"2018-01-01\"]\n",
    "    df_test = this_df[this_df[\"Date\"] >= \"2018-01-01\"]\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if len(df_test) == 0 or len(df_train)==0: pass\n",
    "    \n",
    "    cv = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\", analyzer=\"word\", max_df=0.8)\n",
    "\n",
    "    y_train = df_train[\"Label\"]\n",
    "    y_test = df_test[\"Label\"]\n",
    "\n",
    "    X_train_vect = df_train[\"text_label\"]\n",
    "    X_test_vect = df_test[\"text_label\"]\n",
    "\n",
    "    X_train_dtm = cv.fit_transform(X_train_vect)\n",
    "    X_test_dtm = cv.transform(X_test_vect)\n",
    "\n",
    "    remaining_feats = np.array(df_train[['word_count', 'sentiment_txtblob', 'positivity_sentiment_nltk',\n",
    "                    'compound_sentiment_nltk', 'negativity_sentiment_nltk', 'neutral_sentiment_nltk']])\n",
    "    remaining_test_feats = np.array(df_test[['word_count', 'sentiment_txtblob', 'positivity_sentiment_nltk',\n",
    "                    'compound_sentiment_nltk', 'negativity_sentiment_nltk', 'neutral_sentiment_nltk']])\n",
    "\n",
    "    X_train_dtm = hstack(([X_train_dtm, remaining_feats]))\n",
    "    X_test_dtm = hstack(([X_test_dtm, remaining_test_feats]))\n",
    "\n",
    "    BNB = BernoulliNB()\n",
    "    BNB.fit(X_train_dtm, y_train)\n",
    "\n",
    "    LogReg = LogisticRegression()\n",
    "    LogReg.fit(X_train_dtm, y_train)\n",
    "\n",
    "    SGD = SGDClassifier()\n",
    "    SGD.fit(X_train_dtm, y_train)\n",
    "\n",
    "    SVC_c = SVC()\n",
    "    SVC_c.fit(X_train_dtm, y_train)\n",
    "\n",
    "    ## TEST PREDICTIONS\n",
    "    svc_pred = SVC_c.predict(X_test_dtm)\n",
    "    bnb_pred = BNB.predict(X_test_dtm)\n",
    "    logreg_pred = LogReg.predict(X_test_dtm)\n",
    "    sgd_pred = SGD.predict(X_test_dtm)\n",
    "\n",
    "    ## TRAINING PREDICTIONS\n",
    "    svc_pred_train = SVC_c.predict(X_train_dtm)\n",
    "    bnb_pred_train = BNB.predict(X_train_dtm)\n",
    "    logreg_pred_train = LogReg.predict(X_train_dtm)\n",
    "    sgd_pred_train = SGD.predict(X_train_dtm)\n",
    "\n",
    "\n",
    "    ensemble_pred_test = np.add(svc_pred, bnb_pred + logreg_pred + sgd_pred)/4\n",
    "    ensemble_pred_train = np.add(svc_pred_train, bnb_pred_train + logreg_pred_train + sgd_pred_train)/4\n",
    "\n",
    "    this_pred_test = pd.DataFrame({ticker: list(map(lambda x: 1 if x>= 0 else -1, ensemble_pred_test))})\n",
    "    this_pred_train = pd.DataFrame({ticker: list(map(lambda x: 1 if x>= 0 else -1, ensemble_pred_train))})\n",
    "\n",
    "    ## merge this_pred_train with df_train and this_pred_test with df_test (dates only)\n",
    "    this_pred_train.set_index(df_train[\"Date\"], inplace=True, drop=True)\n",
    "    this_pred_test.set_index(df_test[\"Date\"], inplace=True, drop=True)\n",
    "\n",
    "    ## Make it daily\n",
    "    test_dates = pd.DataFrame(index=pd.date_range(start=\"2018-01-01\", end=\"2019-12-31\", freq=\"D\"))\n",
    "    train_dates = pd.DataFrame(index=pd.date_range(start=\"2010-01-01\", end=\"2017-12-31\", freq=\"D\"))\n",
    "\n",
    "    test_df = pd.merge(test_dates, this_pred_test, how='outer', left_index=True, right_index=True)\n",
    "    test_df.fillna(method=\"ffill\", limit=2, inplace=True)\n",
    "    test_df.fillna(0, inplace=True)\n",
    "\n",
    "    train_df = pd.merge(train_dates, this_pred_train, how='outer', left_index=True, right_index=True)\n",
    "    train_df.fillna(method=\"ffill\", limit=2, inplace=True)\n",
    "    train_df.fillna(0, inplace=True)\n",
    "\n",
    "    ## Remove Weekends\n",
    "    train_df = train_df[train_df.index.dayofweek < 5]\n",
    "    test_df = test_df[test_df.index.dayofweek < 5]\n",
    "    \n",
    "    train_df.index.rename(\"Date\", inplace=True)\n",
    "    test_df.index.rename(\"Date\", inplace=True)\n",
    "\n",
    "    train_df.to_csv(\"../../Predictions/Financial News/\" + ticker + \"_train.csv\")\n",
    "    test_df.to_csv(\"../../Predictions/Financial News/\" + ticker + \"_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSCO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JNJ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NKE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WBA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AXP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksjag\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    if ticker in [\"DOW\", \"TRV\", \"DIS\"]: continue\n",
    "    print(ticker)\n",
    "    \n",
    "    generate_train_test_csv(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT\n",
      "0\n",
      "0\n",
      "WMT\n",
      "0\n",
      "0\n",
      "PG\n",
      "0\n",
      "0\n",
      "VZ\n",
      "0\n",
      "0\n",
      "V\n",
      "0\n",
      "0\n",
      "AAPL\n",
      "0\n",
      "0\n",
      "MMM\n",
      "0\n",
      "0\n",
      "MRK\n",
      "0\n",
      "0\n",
      "CSCO\n",
      "0\n",
      "0\n",
      "UNH\n",
      "0\n",
      "0\n",
      "JNJ\n",
      "0\n",
      "0\n",
      "XOM\n",
      "0\n",
      "0\n",
      "NKE\n",
      "0\n",
      "0\n",
      "IBM\n",
      "0\n",
      "0\n",
      "CAT\n",
      "0\n",
      "0\n",
      "CVX\n",
      "0\n",
      "0\n",
      "WBA\n",
      "0\n",
      "0\n",
      "PFE\n",
      "0\n",
      "0\n",
      "KO\n",
      "0\n",
      "0\n",
      "AXP\n",
      "0\n",
      "0\n",
      "INTC\n",
      "0\n",
      "0\n",
      "BA\n",
      "0\n",
      "0\n",
      "HD\n",
      "0\n",
      "0\n",
      "MCD\n",
      "0\n",
      "0\n",
      "GS\n",
      "0\n",
      "0\n",
      "UTX\n",
      "0\n",
      "0\n",
      "JPM\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    if ticker in [\"DOW\", \"TRV\", \"DIS\"]: continue\n",
    "    print(ticker)\n",
    "    \n",
    "    train = pd.read_csv(\"../../Predictions/Financial News/\" + ticker + \"_train.csv\")\n",
    "    test = pd.read_csv(\"../../Predictions/Financial News/\" + ticker + \"_test.csv\")\n",
    "\n",
    "    print(len(train[train.duplicated(subset=\"Date\") == True]))\n",
    "    print(len(test[test.duplicated(subset=\"Date\") == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = \"AAPL\"\n",
    "train = pd.read_csv(\"../../Predictions/Financial News/\" + ticker + \"_train.csv\")\n",
    "test = pd.read_csv(\"../../Predictions/Financial News/\" + ticker + \"_test.csv\")\n",
    "\n",
    "len(train[train.duplicated(subset=\"Date\") == True])\n",
    "len(test[test.duplicated(subset=\"Date\") == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
